# Scopes Feature PRD

## Problem

Modern AI assistants can utilize user-specific context to provide personalized and efficient responses. However, without clear **memory boundaries**, assistants might inappropriately mix personal, work, or other contexts, leading to privacy risks and irrelevant or intrusive answers. Users need a way to control what portions of their past interactions or profile an assistant can recall in a given conversation. The absence of such control makes it hard for users to trust that the assistant won’t pull in sensitive personal details during a work query, or vice versa. In short, we must enable **selective memory usage** so the assistant “knows you” on your terms without overstepping into unwanted areas.

## Goals

- **Fine-Grained Memory Control:** Allow users to define and toggle on/off distinct memory **Scopes** (e.g. Work, Health, Personal) that limit what background information the assistant can access in a conversation. This gives users confident control over the assistant’s knowledge sources.
- **Privacy by Default:** Ensure that when no scopes are selected, the assistant operates **statelessly** with zero personal recall. The system should favor privacy – no user data is used unless explicitly permitted.
- **Personalization with Boundaries:** Improve the assistant’s usefulness by letting it recall relevant details *only* from the user-approved scopes. This personalization should feel seamless yet maintain **clean boundaries** between contexts.
- **Transparency & Trust:** Make it clear to the user when and what stored information is being used. The assistant (or UI) should indicate the source Scope of any recalled memory (e.g. showing a label like “Used: Health • Mar 3, 2025”) so users aren’t surprised by where an answer came from.
- **Scope as Profile:** Treat the user’s profile (a brief “about me” summary of identity, preferences, etc.) as just another Scope. The profile can be included or excluded just like any other context scope, giving the user control over when their personal identity info is factored in.
- **Seamless Experience Across Models:** Ensure that Scope settings persist and remain effective even if the user switches AI models or providers mid-conversation. The conversation’s context continuity and scope-based constraints should carry over regardless of which model is replying.

## Non-Goals (Out of Scope)

- **Memory Storage Implementation:** This PRD does not prescribe how the underlying memory is stored or retrieved (e.g. vector databases, embeddings, or file system). Low-level implementation of the memory index, summarization techniques, or data schema is beyond the scope of this document.
- **Team/Shared Scopes:** We will not address sharing scopes between users or any multi-user collaboration features. Scopes are personal and user-specific in this feature set (no “team Work scope” or similar in this version).
- **Tool Integration & Token Optimization:** Decisions about specialized tool usage per scope (e.g. different retrieval tools) or fine-tuning token budgets and model-specific optimizations are not included. We assume a working retrieval mechanism but do not define how to optimize it beyond basic constraints.
- **Non-Text Modalities:** Managing scope-specific memory for images, voice notes, or other non-text data is not considered at this stage. Scopes apply to textual memory content from prior chats/notes.
- **Automated Scope Assignment:** We are not implementing automatic categorization of content into scopes. It is assumed that users (or the system via separate features) organize their data into the scopes; this feature focuses only on using those scopes in context.

## User Stories

- **US1:** *As a user, I want to run a conversation with only my **Work** Scope enabled,* so that the assistant draws on my work-related notes and history but nothing from my personal or health data.
- **US2:** *As a user, I want to exclude my **Profile** Scope from a conversation,* so that the assistant does not incorporate my personal identity or background info in its responses for that session.
- **US3:** *As a user, I want to see which Scope a recalled memory came from and be able to remove or disable that Scope if I choose,* so that I have full transparency and control over the assistant’s use of my data.
- **US4:** *As a user, I want to start a new chat with no Scopes enabled,* ensuring the assistant has no access to any past memories for maximum privacy (a completely fresh, context-less session).

## Functional Requirements

- **Scope Selection UI:** Provide an intuitive UI element (e.g. a set of toggle buttons or checkboxes, aka a “persona switcher”) that lets the user enable or disable each available Scope with a single click. The UI should be accessible at the start of a chat and adjustable at any point during the conversation.
- **Multiple Scope Support:** The user may select zero, one, or multiple Scopes at once for a given conversation. The system will compute the *active scope set* as the union of all Scopes currently enabled (including the Profile scope, if selected). Only content from these active scopes is allowed to be recalled by the assistant.
- **Zero-Scopes Behavior:** If no scopes are selected, the assistant should behave as if it has no memory of the user at all. In this state, the assistant **must not** retrieve or use any stored personal data or conversation history beyond the current session’s messages. Essentially, the assistant functions with **empty memory** (apart from the ongoing conversation context) to maximize user privacy.
- **Scoped Recall Only:** The assistant’s memory retrieval mechanism will **only** draw from the data within the enabled scopes. For example, if only “Work” and “Personal” are enabled, any long-term memory or background info injected into the prompt must come from those scopes *and no others*. If a scope is disabled, its data is completely off-limits to the assistant. This constraint must be enforced even if the data might be relevant – the assistant should act as though anything in a disabled scope doesn’t exist unless the user decides to re-enable it.
- **Mid-Conversation Toggling:** Changes to scope selection take effect immediately. If a user enables an additional scope mid-thread, the assistant can begin recalling from it on the next turn. Conversely, if a user disables a scope during a conversation, the assistant must stop retrieving from that scope right away. (Any content already pulled in from that scope in earlier turns remains in the conversation history, but no new content from it should be added unless re-enabled.)
- **Profile as Scope:** The user’s Profile (a concise “about me” summary of personal details, preferences, or instructions) is implemented as a Scope in the same way as Work/Health/Personal. The Profile scope is optional and off by default unless the user toggles it on for a conversation. When enabled, the profile’s info will be injected into the conversation context (typically as a brief system message or memory snippet) just like any other scope’s data.
- **Scope-Specific Policies:** Each Scope can have its own content policy constraints which the assistant must obey. Before any information from a Scope is shown to the user or sent to the model, it should pass through that scope’s policy filter. For example, the Health scope might automatically redact personal identifiers (names, locations) from medical notes, or forbid certain sensitive data from being disclosed. The system must enforce these rules **per scope** consistently, ensuring that no policy-violating data is exposed even if the scope is enabled.
- **Transparency of Recall:** The system should make it transparent what memory has been recalled from which scope. When the assistant uses recalled information, the UI should indicate the scope (or scopes) of origin. For instance, an answer containing a recalled note could include a small annotation like “(Source: Work Scope, Apr 5 2025)” or a UI element showing which scope was used. Additionally, a user should be able to click a “Why did you recall this?” detail to see meta-information such as the source scope and date of the recalled snippet. This builds trust by letting users verify and audit the assistant’s use of their data.
- **Scope Removal & Refinement:** Along with transparency, the UI should allow easy refinement of scope usage. If a user sees that the assistant pulled in a fact from, say, the Health scope that they’d prefer not to use, they should be able to quickly remove or disable that scope and then re-ask or refine the query. This could be achieved via the same toggle UI or a one-click action like “Exclude Health” in response to seeing it was used. Likewise, if the assistant’s answer could benefit from another scope, the user could enable it and prompt the assistant to include more from that scope (e.g. “Include more from Work” quick action).
- **Persistence Across Model Switches:** If the platform allows switching the underlying AI model or provider mid-conversation (e.g. from GPT-4 to Claude 3.5), the active Scope selections must persist through the switch. The system should continue to honor the same enabled scopes when building the context for the new model. In practice, this means the memory retrieval and assembly pipeline is model-agnostic – it produces the same scoped context bundle regardless of which model is responding. The user should not have to re-select scopes after a model switch, and there should be no change in what information is recalled.
- **Consent for Scope Use:** For privacy compliance and user empowerment, the first time a user tries to enable a given scope (especially ones containing sensitive data like Health or a Profile) in a session, the system should prompt them for explicit consent. For example, enabling the Health scope might trigger a one-time pop-up or toast: “Allow assistant to use your Health notes in this conversation?” which the user must accept. Only upon consent will the scope’s content be accessible. This consent should be required per scope (and perhaps per new conversation or after a long time of inactivity) so that users are always making a conscious choice to include potentially sensitive data.
- **Session Persistence:** Scope selections are tied to the conversation session. If the product supports saved conversations or multiple threads, the scopes enabled in each thread should be saved as part of that thread’s state. Reloading or returning to that thread should remember which scopes were on. New conversations should start with a default scopes state (see **Open Questions** on what the default should be), and the user can then choose the scopes for that session.

## Acceptance Criteria

- **No Memory with 0 Scopes:** In a conversation where the user has zero scopes enabled, the assistant’s responses should clearly use no outside memory. Testing this: with all scopes off, ask questions that normally would trigger personal context (e.g. “What’s my next appointment?” when you have such info in a scope) and confirm the assistant either asks for clarification or indicates it doesn’t have that info, rather than retrieving it. The UI should also show no scope usage (no scope labels on answers, no memory sources listed) in this state.
- **Scoped Recall Accuracy:** With a given scope enabled, the assistant should be able to pull relevant info from that scope *and no others*. For example, if only the Work scope is on and the user asks a question that has answers in both Work and Personal data, the assistant’s answer should reflect only the Work data. Any attempt to retrieve from a disabled scope is a failure. In testing, enabling/disabling scopes will be verified by checking that responses only contain content from allowed scopes.
- **Quick Toggle UX:** The scope toggle control in the UI must be easily accessible and operable with one click per scope. Success is measured by user testing where users can successfully turn scopes on or off without confusion. Toggling a scope on should immediately reflect in the UI state (e.g. the scope gets a checkmark or highlight), and toggling off should remove any indication of that scope’s active status. No page refresh or complex menu navigation should be needed to change scopes.
- **Consent Prompt Shown:** When a user first enables a sensitive scope (e.g. the first time ever, or first time in a new chat), a consent or warning prompt should appear. Acceptance is met if in a new session the very first toggle of (for example) the Health scope triggers a confirmation dialog explaining what enabling that scope means, and proceeds only if the user confirms. If the user declines, the scope remains off and no data is used. This prompt should not recur every single time once given (unless a long period or new thread – detail to be determined), but it must happen at least initially for each scope.
- **Policy Enforcement:** The system must enforce redaction and policy rules on recalled content before it reaches the model or the user. As acceptance tests, we will include intentionally sensitive data in a scope (like a Health note with a person’s name or a Work note with an API key) and enable that scope. If the assistant’s answer uses those notes, any disallowed details (the person’s name, the raw API key, etc.) should be filtered out or masked according to the scope’s policy. The assistant should not violate scope-specific rules in any response. A conversation log review should show that the injected prompt content had the proper redactions applied.
- **Transparent Source Attribution:** Whenever the assistant uses recalled information, the UI should clearly indicate the scope source. This criterion is met if, for example, the assistant answers “Your blood pressure is normal today” and the UI reveals (through an inline label or a hover tooltip/popover) that this fact came from the Health scope (and possibly which date or note). Users in a beta test should consistently identify which scope a piece of info came from. Additionally, after seeing this, the user should be able to take action (e.g. disable that scope if they don’t want such info used). We’ll accept the feature when users report that they understand the source of the assistant’s knowledge and feel in control to turn scopes on/off in response.
- **Mid-Thread Scope Change Behavior:** It should be verified that enabling or disabling a scope in the middle of a conversation immediately influences the next assistant response. If a user disables a scope and then asks a follow-up question, the answer should have no content from the now-disabled scope. Conversely, if they enable a new scope and ask a question that relates to that scope’s data, the assistant should successfully incorporate relevant info from it. The transition should not produce errors or require restarting the conversation. This will be acceptance-tested by scenario: start with a scope off, ask a question (assistant can’t answer fully), then enable the scope and ask again – the assistant should now use the new info, demonstrating the toggle effect.
- **Scope Persistence with Model Switch:** In a conversation, after some turns using certain scopes, if the user switches the AI model/provider, the next response (from the new model) should still respect the previously selected scopes. We will test by starting a chat with a given scope on, getting an answer, then switching from Model A to Model B and asking a follow-up. The result passes if Model B’s answer continues to draw only from the allowed scopes and the scope toggles remain as set (no resets). There should be no need for the user to re-select or confirm scopes after switching models.
- **No Cross-Scope Leakage:** There should be no bleed of information between scopes when not intended. For example, if both Work and Personal scopes are on and the user asks something domain-specific to one scope, the assistant shouldn’t unnecessarily pull irrelevant data from the other. More critically, if Personal is off and Work is on, none of the personal details should leak into answers. Passing this criterion involves code review or instrumentation to confirm that the retrieval function truly limits results to enabled scopes, and running test queries where data exists in a disabled scope (to ensure it’s ignored).
- **Robust Performance:** The addition of Scopes should not significantly degrade the assistant’s response time or reliability. As an acceptance benchmark, with a reasonable amount of data in each scope, the assistant’s average latency should remain within, say, ~500ms of the latency with scopes disabled (exact target to be determined, but e.g. if base response is 2s, with scopes it might be 2.5s). We will measure a few scenarios (no scopes vs. multiple scopes enabled with recalls) to ensure the overhead of retrieval and filtering is within acceptable range. The feature is accepted if users do not perceive a major slowdown and all responses still succeed within timeouts.

## Constraints

- **Context Window Limits:** The language model has a finite context length, so the system must be careful about how much scoped memory to include. We cannot dump entire scope histories into each prompt. Instead, the assistant should inject only the most relevant snippets or summaries from the enabled scopes. Aim to keep total prompt size (conversation + retrieved context) under a threshold (e.g. ~8k tokens for most cases). This may require summarizing long past chats or selecting top-K relevant pieces rather than everything. Staying within context limits is essential to avoid model truncation or high latency.
- **Privacy & Security:** All user memory data associated with scopes should be stored and handled securely. The design should assume this data might be sensitive (especially for scopes like Health or Personal). As such, data at rest should be encrypted if stored on-device or on-server, and only decrypted/used in-memory when building a prompt for an enabled scope. Moreover, the system should never send data from a scope to the model unless that scope is active and user has given consent. This implies that the *default* state for new conversations could be no scopes active, requiring user action to include any personal data. We constrain the design to be “privacy-first” – no hidden use of user context.
- **User Experience Simplicity:** Introducing scopes adds complexity, so it must be done in an intuitive way. The UI must remain clean and not overwhelm the user. We are constrained to simple interactions (e.g. a small set of toggle chips or switches) rather than complicated menus. The concept of scopes should be explained or evident to the user (possibly via onboarding or tooltip) so they understand what enabling “Work” means. This is a soft constraint, but critical for adoption: if users are confused by the feature, they may avoid using it and lose out on benefits.
- **Consistency Across Platforms:** If the assistant is available across multiple platforms (web, mobile, etc.) or if the user switches AI models, the Scopes feature should work uniformly. The active scopes set should be consistently applied irrespective of where or how the conversation continues. This requires that scope filtering logic is implemented at a layer independent of any specific model – effectively as part of our application’s context assembly pipeline.
- **Performance Overhead:** The retrieval and filtering of scope data should be efficient. We target minimal added latency (e.g. on the order of tens of milliseconds for retrieving relevant snippets from local storage or an index) so that user experience remains snappy. If necessary, we’ll use caching of summaries or embeddings to speed this up. The system should also handle cases where a scope contains a lot of data by limiting search space (possibly by maintaining per-scope indexes). The constraint is that enabling scopes shouldn’t make the assistant noticeably sluggish or cause timeouts.
- **Scalability of Scopes:** The design should support an arbitrary number of scopes conceptually (since they are user-defined), but we expect most users to have a handful (e.g. 2-5) major scopes. The UI and system must handle if a user creates many scopes, but we won’t deeply optimize for, say, 50+ scopes in this iteration. We assume a reasonable upper bound (and can set guidelines in UI to prevent scope proliferation beyond what the interface can neatly display).
- **Data Freshness and Persistence:** It’s assumed that each scope’s data (e.g. past chats or notes in that category) is maintained separately. The system should ensure that when a scope is enabled, the data used is relatively up-to-date (e.g. using the latest summary of a recent chat, not a stale version). However, real-time updating of scope content is an implementation detail. A constraint here is that if scope content changes (user edits a note, or adds a new chat to a scope), the recall mechanism should either incorporate the update or at least not use obsolete info. This likely means we need a strategy for updating or invalidating cached summaries (an implementation concern to be aware of).
- **No Implicit Scope Assumptions:** The assistant should not assume context that wasn’t provided. For example, if only Work scope is on, it shouldn’t bring in personal info just because it “knows” the user profile from another session. This is a constraint on model instructions: the model will only be given the data from selected scopes, and it should be instructed to stick to that. (We might include a system note like “only use Work and Personal scopes this session” to reinforce this.) This ensures the model doesn’t inadvertently leak or use data beyond the allowed set.
- **Regulatory Compliance:** Especially for scopes like Health, the feature should help compliance with privacy laws (HIPAA, GDPR, etc.). This means auditability (knowing which data was used when), explicit consent logs, and easy user controls to delete or exclude data. While full compliance implementation is beyond this PRD’s scope, as a constraint we acknowledge that the Scopes concept is intended to enhance compliance by design (data minimization and user consent for data use).

## Open Questions

- **Default Scope State:** What should be the default scopes selection when a user starts a brand new conversation? One approach is *starting with zero scopes* enabled to maximize privacy (user opt-in each time). Another approach could be enabling a default set (e.g. Profile + maybe one other frequently used scope) to improve convenience. We need to decide which default balances privacy and user convenience, or possibly allow the user to configure a default. This is still under consideration.
- **Scope Creation & Management:** How do users create and manage their Scopes? Are we providing a UI in settings to define new scopes (name, description, included data) and to assign conversations or notes to scopes? It’s implied that scopes are user-defined, but the PRD above doesn’t detail that process. We need to clarify if the initial release assumes a fixed set of scopes (Work, Personal, Health, Profile) or a fully dynamic user-created set. The complexity of scope management (adding, renaming, deleting scopes) and how that ties into data categorization is an open design area.
- **Granularity of Recall:** When multiple scopes are enabled, should the assistant be aware of which scope is more relevant or prioritize one if there’s conflicting info? For example, if “Work” and “Personal” are both on and the same question has answers in both, does it merge info, prefer one, or ask for clarification? The retrieval system will likely pull from all enabled scopes, but we might need rules for conflict resolution or prioritization. We haven’t fully answered how to handle overlapping or contradictory information coming from different scopes.
- **Mid-Conversation Scope Change Edge Cases:** If a user disables a scope that was previously used in the conversation, how should the assistant handle knowledge already revealed from that scope? We cannot make the model “forget” what was already said in earlier turns, but we could instruct it not to use or mention that content going forward. Do we need to implement any specific behavior (like alerting the user “We will stop using data from X going forward” or scrubbing the context) when this happens? This is an open question especially for sensitive info – simply turning off a scope doesn’t erase what’s already in the transcript, so we may need to document this limitation or find a mitigation strategy.
- **Temporary Scope Enabling:** Should we allow users to enable a scope temporarily for a single question or a short time window? The idea was mentioned (e.g. “enable Health for the next 30 minutes” as a temporary scope). This could prevent accidentally leaving a sensitive scope on. We haven’t decided if this level of granularity will be offered in the UI, or if it complicates the model input handling (would need timers or automatic disable). It remains an open consideration for future versions.
- **User Education and Onboarding:** How will we explain the Scopes feature to users so that it’s used correctly? Terms like “Work scope” or “Personal scope” might need explanation. We might consider a brief tutorial or tooltips (especially to highlight the privacy benefits of having 0 scopes vs the personalization trade-off). Deciding on the right terminology (Scope vs Persona vs Context Group) is also on the table. This is not a technical requirement but an open question for the product experience team to ensure the concept is clear to end users.
- **Integration with Future Features:** If we later introduce tools or plugins (e.g., calendar access, email summary, etc.), how will Scopes interact with those? For instance, a “Work” scope might naturally tie in with a user’s work calendar or documents. We have explicitly left tools out-of-scope for now, but we should keep in mind whether scopes will eventually also gate tool usage (e.g. only use certain tools in certain scopes) or if tools get their own separate permission system. It’s an open design question how this feature will extend as the assistant gains more capabilities.
- **Scope Data Volume Limits:** Do we impose any limits on how much data a single scope can hold or how far back it can recall? If a user has years of chat history in a Work scope, the retrieval system will summarize/search it, but should we set any practical limits (like only last 6 months of data are considered, or a maximum number of notes)? This may be answered during implementation when we see performance implications, but it remains a question now.
- **Testing and Tuning Recall:** How will we measure that the assistant is effectively using scopes and not missing important info? We might need to develop specific test cases or even fine-tune retrieval scoring for each scope. For example, ensuring Health questions prefer Health scope data. It’s an open question how much “intelligence” to add in selecting scope-relevant info vs. relying on vector similarity. User feedback will be crucial here.
- **Failure Modes and Fallbacks:** What happens if a scope is enabled but no relevant info is found in it? The assistant should then either ask for clarification or answer based on general knowledge, but we need to confirm the behavior. Also, if the retrieval system fails (e.g. index error) or returns garbage, how does the assistant handle that gracefully without exposing raw data or confusing the user? Defining fallback behavior (perhaps treat it as no-scope scenario temporarily, or apologize and continue) is an open item for error handling strategy.
- **Consent Granularity:** We have the idea of a one-time consent per scope. We should decide if the consent is once per user per scope, once per thread, or something else. For instance, a user might consent to use Health scope in one conversation but not want it automatically assumed in another. How persistent is that consent? Perhaps we need a per-session consent, or a global setting “Always allow Health scope when I enable it.” This is a privacy/legal question as well as a UX one that we need to resolve.
- **Metrics and Logging:** What metrics will we track to evaluate the Scopes feature’s success and usage? Possible metrics: number of conversations with scopes enabled, average number of scopes enabled per conversation, how often users toggle scopes during a chat, and if users tend to default to zero scopes or not. We should also consider logging when recalled data is used (perhaps at a high level, respecting privacy) to detect any misuse or errors in scope isolation. Deciding on these metrics and ensuring they don’t violate privacy (e.g. not logging actual content, just counts) is an open question for our analytics plan.